# Pipeline ETL Grand Livre Comptes

## üéØ Objectif du projet

Automatiser la transformation de fichiers Excel "Grand Livre Comptes" en fichiers structur√©s (JSON + Excel) pr√™ts √† l'emploi.

## üîÑ Fonctionnement g√©n√©ral

```
üìÇ Fichier Excel d√©pos√©
         ‚Üì
   üîç D√©tection automatique (Airflow)
         ‚Üì
   ‚öôÔ∏è Transformation des donn√©es
         ‚Üì
   üíæ G√©n√©ration JSON + Excel
         ‚Üì
   ‚úÖ Fichiers pr√™ts dans output/
```

## üì¶ Architecture du projet

### 1Ô∏è‚É£ **Airflow (Orchestrateur)**
- **R√¥le** : Surveille, planifie et ex√©cute le pipeline
- **Quand** : Toutes les 5 minutes (configurable)
- **O√π** : http://localhost:8080

### 2Ô∏è‚É£ **DAG (Workflow)**
- **R√¥le** : D√©finit les √©tapes du pipeline
- **Fichier** : `dags/etl_excel_pipeline.py`
- **√âtapes** :
  1. D√©tecter les fichiers Excel
  2. Transformer les donn√©es
  3. Afficher le r√©sum√©

### 3Ô∏è‚É£ **Script de transformation**
- **R√¥le** : Lit l'Excel et extrait les donn√©es comptables
- **Fichier** : `scripts/transform_excel.py`
- **Actions** :
  - Parse le format Grand Livre
  - Extrait les comptes et transactions
  - G√©n√®re JSON + Excel

### 4Ô∏è‚É£ **Dossiers de donn√©es**
- **`data/input/`** : D√©p√¥t des fichiers Excel √† traiter
- **`data/output/`** : Fichiers transform√©s (JSON + Excel)
- **`data/input/processed/`** : Archive des fichiers trait√©s

## üìã D√©tail des fichiers

### `dags/etl_excel_pipeline.py`
**Le cerveau du pipeline**

```python
# D√©finit 2 t√¢ches principales:
# 1. D√©tecter et traiter les fichiers
# 2. Afficher le r√©sum√©
```

**Param√®tres importants :**
- `schedule_interval='*/5 * * * *'` ‚Üí Fr√©quence d'ex√©cution (5 min)
- `INPUT_DIR` ‚Üí Chemin du dossier d'entr√©e
- `OUTPUT_DIR` ‚Üí Chemin du dossier de sortie

### `scripts/transform_excel.py`
**Le moteur de transformation**

**Fonctions principales :**

1. **`parse_grand_livre_comptes()`**
   - Lit le fichier Excel
   - D√©tecte l'entit√©, la p√©riode
   - Extrait les comptes (num√©ro √† 6 chiffres + libell√©)
   - Extrait les transactions (date + journal + montants)

2. **`save_outputs()`**
   - G√©n√®re le fichier JSON structur√©
   - G√©n√®re le fichier Excel plat

3. **`transform_file()`**
   - Fonction principale qui orchestre tout

**Format d'entr√©e attendu :**
```
Ligne avec: [Num√©ro compte 6 chiffres] [null] [Libell√© compte]
  Transactions:
    [Date DDMMYY] [Code Journal] [N¬∞ Pi√®ce] ... [D√©bit col 11] [Cr√©dit col 14] [Solde col 17]
```

### `docker-compose.yml`
**Configuration de l'environnement**

**Services lanc√©s :**
- `postgres` : Base de donn√©es pour Airflow
- `airflow-webserver` : Interface web (port 8080)
- `airflow-scheduler` : Ex√©cuteur des t√¢ches
- `airflow-init` : Initialisation (premi√®re fois)

**Volumes mont√©s :**
- `./dags` ‚Üí `/opt/airflow/dags` (vos DAGs)
- `./scripts` ‚Üí `/opt/airflow/scripts` (vos scripts)
- `./data` ‚Üí `/opt/airflow/data` (vos donn√©es)

### `requirements.txt`
**D√©pendances Python**

- `apache-airflow` : Orchestrateur
- `pandas` : Manipulation de donn√©es
- `openpyxl` : Lecture/√©criture Excel

## üé¨ Cycle de vie d'un fichier

### 1. D√©p√¥t
```bash
cp mon_grand_livre.xlsx data/input/
```

### 2. D√©tection (toutes les 5 min)
Airflow v√©rifie `data/input/` et d√©tecte `mon_grand_livre.xlsx`

### 3. Transformation
```
mon_grand_livre.xlsx
  ‚Üì Lecture avec openpyxl
  ‚Üì Extraction m√©tadonn√©es (entit√©, p√©riode)
  ‚Üì Parsing comptes (6 chiffres + libell√©)
  ‚Üì Parsing transactions (date + montants)
  ‚Üì Structure des donn√©es
```

### 4. G√©n√©ration
Cr√©e 2 fichiers dans `data/output/` :
- `mon_grand_livre_20251008.json`
- `mon_grand_livre_20251008.xlsx`

### 5. Archivage
Le fichier original est d√©plac√© vers `data/input/processed/`

## üìä Format des fichiers de sortie

### JSON (structure hi√©rarchique)
```json
[
  {
    "Numero_Compte": "512000",
    "Libelle_Compte": "Banque",
    "Periode": "202412",
    "Transactions": [
      {
        "Date_GL": "31/12/2024",
        "Entite": "ENVOL",
        "Compte": "512000",
        "Date": "01/12/2024",
        "Code_Journal": "BQ",
        "Numero_Piece": "001",
        "Libelle_Ecriture": "Virement client",
        "Debit": 1000.0,
        "Credit": 0.0,
        "Solde": 1000.0
      }
    ]
  }
]
```

### Excel (tableau plat)
Chaque ligne = 1 transaction avec toutes les infos :

| Numero_Compte | Libelle_Compte | Periode | Date | Debit | Credit | Solde |
|---------------|----------------|---------|------|-------|--------|-------|
| 512000 | Banque | 202412 | 01/12/2024 | 1000 | 0 | 1000 |

## ‚öôÔ∏è Configuration

### Modifier la fr√©quence d'ex√©cution

Dans `dags/etl_excel_pipeline.py` :

```python
# Toutes les 5 minutes (d√©faut)
schedule_interval='*/5 * * * *'

# Toutes les 15 minutes
schedule_interval='*/15 * * * *'

# Toutes les heures
schedule_interval='0 * * * *'

# Une fois par jour √† 9h
schedule_interval='0 9 * * *'

# D√©sactiver l'auto (manuel uniquement)
schedule_interval=None
```

### Modifier les chemins

Dans `dags/etl_excel_pipeline.py` :

```python
INPUT_DIR = "/opt/airflow/data/input"   # Chemin dans Docker
OUTPUT_DIR = "/opt/airflow/data/output"

# Ces chemins correspondent √†:
# ./data/input/  sur votre machine
# ./data/output/ sur votre machine
```

## üîç Monitoring

### Via l'interface Airflow (http://localhost:8080)

1. **Vue d'ensemble** : Liste de tous les DAGs
2. **Graph View** : Visualisation du workflow
3. **Logs** : D√©tails d'ex√©cution de chaque t√¢che
4. **Runs** : Historique des ex√©cutions

### Via les logs Docker

```bash
# Logs en temps r√©el du scheduler
docker-compose logs -f airflow-scheduler

# Logs d'une t√¢che sp√©cifique
# (disponibles aussi dans l'interface web)
```

## üö® Gestion des erreurs

### Si un fichier √©choue
- Le fichier reste dans `data/input/`
- L'erreur est visible dans les logs Airflow
- Les autres fichiers continuent d'√™tre trait√©s

### Si le format est incorrect
- Message d'erreur dans les logs
- V√©rifiez que le fichier respecte le format attendu
- Colonnes d√©bit (11), cr√©dit (14), solde (17)

## üí° Bonnes pratiques

### ‚úÖ √Ä faire
- Tester avec un petit fichier d'abord
- Consulter les logs en cas de probl√®me
- Sauvegarder les fichiers output importants
- Vider r√©guli√®rement `data/input/processed/`

### ‚ùå √Ä √©viter
- Ne pas d√©poser de fichiers non-Excel dans input/
- Ne pas modifier les fichiers pendant le traitement
- Ne pas arr√™ter Docker pendant une ex√©cution

## üìà √âvolutions possibles

### Court terme
- Notification email en cas d'erreur
- Dashboard de statistiques
- Validation du format avant traitement

### Moyen terme
- Interface web pour d√©poser les fichiers
- Export vers base de donn√©es
- G√©n√©ration de rapports automatiques

### Long terme
- Migration vers AWS S3
- API REST pour interroger les donn√©es
- Machine Learning sur les transactions

## üÜò Aide rapide

| Probl√®me | Solution |
|----------|----------|
| DAG non visible | V√©rifier `docker-compose logs airflow-scheduler` |
| Fichier non trait√© | V√©rifier que le DAG est activ√© (ON) |
| Erreur de format | Consulter les logs de la t√¢che dans Airflow |
| Container qui crash | `docker-compose down && docker-compose up -d` |
| Manque de RAM | Augmenter RAM Docker (4 GB minimum) |

## üìû Support

Pour toute question :
1. Consultez les logs dans l'interface Airflow
2. V√©rifiez ce document
3. Examinez les logs Docker : `docker-compose logs`